Extract per-timestep batch:
    batch_dict_t = { k: v[:, t] for k, v in batch_seq.items() }

Per-scene time index:
    for each scene s in this batch:
        local time t_s = t   (0,1,2,… within this sequence)
        - transform into last timestep coordinates
        - either explicit from laoder or implicit innetrwork

TemporalPointPillar.forward_single_timestep(batch_dict_t, t_s)
    ↓
[standard PP stack, frame t of each scene]
    points_{s,t}
        → VFE
        → 3D backbone
        → BEV scatter
        → 2D BEV backbone
            → spatial_features_2d_{s,t} ∈ ℝ^{B×C_bev×H×W}
                ↓
            XMemBEVModule (temporal BEV module)
                inputs:
                    spatial_features_2d_{s,t}
                    scene_token_s  (from batch_dict_t)
                    local time index t_s
                    internal memory + hidden state for scene s
                internal operations (conceptually):
                    frames_lidar_t = spatial_features_2d_{s,t}
                    → XMemBackboneWrapper.forward_step(
                          t_s,
                          frames_lidar_t,
                          scene_masks_{s,t},   (union BEV mask, or GT / predicted)
                          agent_masks_{s,t},   (optional, can be None for detection-only)
                          valids_mask_{s,t}    (optional)
                      )
                      → occ_logits_{s,t} ∈ ℝ^{B×1×H_m×W_m}
                      → resize occ_logits_{s,t} → ℝ^{B×1×H×W}
                      → gate_{s,t} = σ(occ_logits_{s,t})
                      → spatial_features_2d_{s,t}^temporal
                           = spatial_features_2d_{s,t} ⊙ gate_{s,t}
                    updates:
                      memory and hidden state for scene s
                      t_s is implicitly advanced by the outer loop
                output to PP:
                    spatial_features_2d_{s,t}^temporal
                ↓
            replace in batch_dict_t:
                batch_dict_t["spatial_features_2d"] = spatial_features_2d_{s,t}^temporal
    ↓
Dense detection head (PP multihead / AnchorHeadMulti)
    uses spatial_features_2d_{s,t}^temporal
    ↓
    classification_logits_{s,t}, box_regression_{s,t}, dir_logits_{s,t}


The detection head always sees the current frame’s BEV features, modulated by a gate that encodes history.

You backpropagate using the loss for each frame (usually averaged over the sequence).

So your mental model can be:

“At each timestep I run PointPillars → XMem (which injects temporal context via a gate) 
→ detection head. XMem carries memory across timesteps inside the same scene; 
the detector always uses the current timestep’s gated BEV features.”





def build_seq_loader(cfg, logger, workers, seq_len, stride):
    dataset_cfg = cfg.DATA_CONFIG
    class_names = cfg.CLASS_NAMES

    train_set = NuScenesSeqDataset(
        dataset_cfg=dataset_cfg,
        class_names=class_names,
        training=True,
        root_path=None,
        logger=logger,
        seq_len=seq_len,
        stride=stride,
        nusc_version=dataset_cfg.VERSION,
        nusc_dataroot=dataset_cfg.DATA_PATH,
    )

    train_loader = DataLoader(
        train_set,
        batch_size=1,
        shuffle=True,
        num_workers=min(workers, cfg.OPTIMIZATION.NUM_WORKERS),
        pin_memory=True,
        collate_fn=collate_seq,
        drop_last=False,
    )

    return train_set, train_loader

loss over the last or all timesteps? probably only the last
how to stage the training


## **My Recommendation:**

**Start with Option 1 (Ego-Motion Compensation)** - it's:
- ✅ Simple to implement (~50 lines of code)
- ✅ High impact (+2-3% expected)
- ✅ Works with your existing pipeline
- ✅ Can add to both baseline-multiframe AND temporal models

**Implementation Priority:**
```
1. Option 1: Ego-motion compensation (MUST HAVE)
   → Warp previous masks to current frame
   → Huge impact, easy to add

2. Option 2: Ego-velocity features (NICE TO HAVE)
   → Add ego motion as extra BEV channels
   → Small improvement to vel_err

3. Option 3: Motion-aware gating (ADVANCED)
   → Only if Option 1 works well

4. Option 4: Multi-frame fusion (RESEARCH EXTENSION)
   → Could be a separate paper
```

**Expected Results with Option 1:**
```
Baseline (no ego-compensation):
  mAP: 44.6%, NDS: 58.15%

Baseline + Ego-Compensation:
  mAP: 46-47%, NDS: 59-60%  (+2% just from alignment!)

Temporal + Ego-Compensation:
  mAP: 48-50%, NDS: 61-62%  (XMem + alignment = best)



EVAL
python eval.py   --cfg_file xmem_det/configs/temporal_pp_xmem_nuscenes.yaml   --xmem_cfg xmem_det/configs/xmem.yaml  --ckpt log/phase1_ckpt_epoch_6.pth   --split val   --seq_len 8   --stride 8   --alpha 1.0   --workers 4   --log_interval 100   --extra_tag default   --eval_tag win8_stride8

python eval.py \
  --cfg_file xmem_det/configs/temporal_pp_xmem_nuscenes.yaml  --xmem_cfg xmem_det/configs/xmem.yaml \ 
  --ckpt log/phase1_ckpt_epoch_7.pth \
  --split val \
  --seq_len 8 \
  --alpha 1.0 \
  --log_interval 100 \
  --extra_tag default \
  --eval_tag opt1_seq8_val
